---
layout: post
title: "Task Empowered"
categories: misc
---

# Content

- [Trajectory Prediction](#Trajectory Prediction)
- [Image Captioning](#Image Captioning)
- [Social Interaction Detection](#Social Interaction Detection)
- [Image-text Retrieval](#Image-text Retrieval)
- [Background Segmentation](#Background Segmentation)
- [Image-text Generation](#Image-text Generation)

## Trajectory Prediction

### ![pic](https://github.com/xzh0312/minima/blob/master/imgs/Trajectory_Prediction1.png?raw=true)

**Baselines**：
Social LSTM: It is the pioneering data-driven method for the trajectory prediction task. The motion pattern of each pedestrian is modeled with an LSTM. And the hidden states of pedestrians are pooled with neighboring pedestrians at each time-step using a pooling mechanism, namely, Social Pooling, which achieves modeling social interactions.

Social GAN: It is the first generative model based method for the pedestrian trajectory prediction task. Generative Adversarial Network is used to generate more socially-acceptable trajectories. And a new pooling mechanism is proposed to model social interactions. 

STGAT: It is one of the first graph based method for the pedestrian trajectory prediction task. The social interactions between pedestrians are modeled by a graph structure and then captured by the Graph Attention Networks. And it also uses two LSTMs to capture spatial and temporal dynamics.

STAR: It is the first transformer based framework for the trajectory prediction task, which tackles the motion patterns and social interactions by only attention mechanism. STAR captures complex spatio-temporal interactions by interleaving between spatial and temporal Transformers.

### ![pic](https://github.com/xzh0312/minima/blob/master/imgs/Trajectory_Prediction2.png?raw=true)

### ![pic](https://github.com/xzh0312/minima/blob/master/imgs/Trajectory_Prediction3.png?raw=true)
### ![pic](https://github.com/xzh0312/minima/blob/master/imgs/Trajectory_Prediction4.png?raw=true)

Implementation Details
We predict the future 12 timestep trajectories (4.8 sec) given the historical 8 timestep trajectories (3.2 sec). We train the models using Adam optimizer with a learning rate 0.001. And the implementation details of all models are consistent with the baselines.

Analysis
We show the performance of baseline models on CrowdVerse and two widely used public pedestrian trajectory prediction datasets ETH and UCY. Following previous works, we use the same evaluation methodology. We predict the future 12 timestep trajectories (4.8 sec) given the historical 8 timestep trajectories (3.2 sec). And two metrics are used for evaluation: Average Displacement Error (ADE), Final Displacement Error (FDE). The experimental results of quantitative analysis are shown in the tables above. The overall experimental results show that the performances of baseline models on our proposed datasets are much worse than that on existing datasets ETH and UCY. These quantitative experimental results indicate that CrowdVerse is a very large-scale dataset, containing a wide range of scenes and a large number of hard cases with complex interactions. In order to better understand the contribution of CrowdVerse, we visualize three challenging scenes. Case (a) shows a very dense crowd scene, in which the density of pedestrians far exceeds that of the existing datasets ETH and UCY. Case (b) shows a drastic scene, in which the behaviors of dancers change rapidly over time. Case (c) shows a scene from the first-person perspective rather than the top-down perspective and. There three kinds of challenging scenes are not contained in the existing datasets ETH and UCY. The performance of baseline models deteriorates significantly in these three scenes compared to the results of existing datasets ETH and UCY. Hence, our proposed datasets address the limitation of the lack of large-scale datasets with complex interactions for the trajectory prediction task, which can further help to design better models. 


## Image Captioning

### ![pic](https://github.com/xzh0312/minima/blob/master/imgs/Image_Captioning1.png?raw=true)

**Image captioning**
Implementation details
We select 50 samples from the proposed dataset and formulate a meta-evaluation based on off-the-shelf image captioning methods, including BLIP, BLIP-2, SmallCap, ConZIC and LLaVA. Note that the above models consist of an all-round model types ranging from vision-language pre-train model, retrieval-augmentated lightweight model, training-free controllable model and multi-modal large language model.

For evaluation, we utilize two methods to both qualitatively and quantitatively verify the quality of the captions: (1) GPT-4o and human as the judge, to rate each generated caption according to designed prompt and task, (2) CLIPscore, RefCLIPscore and other reference-based metrics: matching score between the generated caption and ground-truth image/text.

Human-judge evaluation scores (1-5):
### ![pic](https://github.com/xzh0312/minima/blob/master/imgs/Image_Captioning2.png?raw=true)

CLIPscore（non-ref）, RefCLIPscore and other metrics:
### ![pic](https://github.com/xzh0312/minima/blob/master/imgs/Image_Captioning3.png?raw=true)

### ![pic](https://github.com/xzh0312/minima/blob/master/imgs/Image_Captioning4.png?raw=true)

### Quantitative Comparison:
BLIP, BLIP2, SMALLCAP: Coarse-grained, with short length and low information content (correlated with the training dataset characteristics). 
ConZIC: Lacks fluency in generation and exhibits poor accuracy (a drawback of increased diversity). 
LLaVa: Moderate information content (with non-specific, ambiguous descriptions), demonstrates logical coherence, but may engage in non-visually grounded reasoning and exhibit repetition between sentences. 
GT (Ground Truth): Rich in information content, clear in structure and logic, combines both coarse-grained and fine-grained details, and provides accurate visual reasoning information. These observations highlight the varying capabilities and limitations of different vision-language models in terms of their output characteristics, information density, and reasoning abilities.

### Experiment Settings:
BLIP: blip-image-captioning-large
BLIP2: blip2-opt-2.7b
SMALLCAP: SmallCap7M with COCO datestore
ConZIC:
 --run_type "caption" --order "shuffle" --sentence_len 10 --caption_img_path "./examples/girl.jpg" --samples_num 3 --lm_model "bert-base-uncased" --match_model "openai/clip-vit-base-patch32"  --alpha 0.02 --beta 2.0 
LLaVa:  llava-v1.5-7b
--temperature 0.2 --top_p None --num_beams 1 --max_new_tokens 512
prompt = "Please generate a description of this image, as detailed as possible."


## Social Interaction Detection

这是第三部分的内容。

## Image-text Retrieval

这是第三部分的内容。

## Background Segmentation

这是第三部分的内容。

## Image-text Generation

这是第三部分的内容。
